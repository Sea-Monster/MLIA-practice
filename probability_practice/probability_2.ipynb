{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 示例：使用朴素贝叶斯过滤垃圾邮件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "朴素贝叶斯的一个最著名应用：电子邮件垃圾过滤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 收集数据：提供文本文件  \n",
    "2. 准备数据：将文本文件解析成词条向量  \n",
    "3. 分析数据：检查词条确保解析的正确性  \n",
    "4. 训练算法：使用之前建立的train_NB1函数  \n",
    "5. 测试算法：使用classify_NB()函数，并且构建一个新的测试函数来计算文档集的错误率  \n",
    "6. 使用算法：构建一个完整的程序对一组文档进行分类，将错分的文档输出到屏幕上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/SeaMonster/PycharmProjects/MLIA-practice/probability_practice/email/spam'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()\n",
    "os.path.join(os.getcwd(),'probability_practice','email','spam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之前建立的函数，搬过来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_NB1(train_matrix, train_category):\n",
    "    \"\"\"\n",
    "    train_NB0函数的改进\n",
    "    :param train_matrix:    训练文档矩阵（准确来说只是python原生二维数组）\n",
    "    :param train_category:  训练文档矩阵对应的分类（一维向量）\n",
    "    :return: (在非侮辱性文档类别下词汇表中单词的出现概率向量, 在侮辱性文档类别下词汇表中单词的出现概率向量, 任意文档属于侮辱性文档的概率)\n",
    "    \"\"\"\n",
    "    # 训练文档的数目\n",
    "    num_train_docs = len(train_matrix)\n",
    "    num_words = len(train_matrix[0])\n",
    "\n",
    "    # 分类为\"侮辱性文档\"的概率: 分类为\"侮辱性文档\"的文档数目，除以训练文档的总数目\n",
    "    # train_category 为一维向量，只有0和1两种值，其中1代表侮辱性\n",
    "    p_abusive = sum(train_category) / float(num_train_docs)\n",
    "\n",
    "    # 初始化概率：初始化为1而不是0\n",
    "    # p0_num: 对于分类0，词汇表中单词的出现次数\n",
    "    # p1_num: 对于分类1，词汇表中单词的出现次数\n",
    "    p0_num = np.ones(num_words)\n",
    "    p1_num = np.ones(num_words)\n",
    "\n",
    "    # 这两个是充当分母，\n",
    "    # TODO 为什么是2？为什么不是1或者其他数？\n",
    "    p0_denom = 2.0\n",
    "    p1_denom = 2.0\n",
    "\n",
    "    for i in range(num_train_docs):\n",
    "        if train_category[i] == 1:  # 类别为\"侮辱\"\n",
    "            p1_num += train_matrix[i]\n",
    "            p1_denom += sum(train_matrix[i])  # 训练文档中的单词同时也在词汇表中存在的数目\n",
    "        else:\n",
    "            p0_num += train_matrix[i]\n",
    "            p0_denom += sum(train_matrix[i])  # 训练文档中的单词同时也在词汇表中存在的数目\n",
    "\n",
    "    # 概率向量(转为对数)\n",
    "    # 分母是，对于分类1（侮辱性文档），一共出现过多少个单词（所有文档中）\n",
    "    # 分子是，对于分类1（侮辱性文档），词汇表中每个单词各自出现的次数（所有文档中）\n",
    "    # 相除后，得到一个对于分类1（侮辱性文档），词汇表中每个单词出现的概率\n",
    "    p1_vector = np.log(p1_num / p1_denom)\n",
    "\n",
    "    # 对每个元素做除法\n",
    "    p0_vector = np.log(p0_num / p0_denom)\n",
    "\n",
    "    return p0_vector, p1_vector, p_abusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_NB(vector_2_classify, p0_vector, p1_vector, p_class_1):\n",
    "    \"\"\"\n",
    "    朴素贝叶斯分类函数\n",
    "\n",
    "    这里边的公式都TMD怎么来的？！\n",
    "\n",
    "    :param vector_2_classify:   要分类的向量\n",
    "    :param p0_vector:           训练集中类别为\"非侮辱性\"的文档中，词汇表中每个单词的出现概率（组成一个一维向量）\n",
    "    :param p1_vector:           训练集中类别为\"侮辱性\"的文档中，词汇表中每个单词的出现概率（组成一个一维向量）\n",
    "    :param p_class_1:           训练集中类别为\"侮辱性\"文档的出现概率\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    p1 = sum(vector_2_classify * p1_vector) + np.log(p_class_1)\n",
    "    p0 = sum(vector_2_classify * p0_vector) + np.log(1.0 - p_class_1)\n",
    "    print('p1:')\n",
    "    print(p1)\n",
    "    print('p0:')\n",
    "    print(p0)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary_list(data_set):\n",
    "    \"\"\"\n",
    "    创建一个包含在所有文档中出现的不重复词的列表\n",
    "    :param data_set:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 创建一个空集\n",
    "    vocabulary_set = set([])\n",
    "    for document in data_set:\n",
    "        # 创建两个集合的并集\n",
    "        vocabulary_set = vocabulary_set | set(document)\n",
    "    return list(vocabulary_set)\n",
    "\n",
    "\n",
    "def set_of_words_2_vector(vocabulary_list, input_set):\n",
    "    \"\"\"\n",
    "    生成词集向量\n",
    "    :param vocabulary_list:     词汇的列表\n",
    "    :param input_set:           某个文档\n",
    "    :return:                    文档向量，向量的每一个元素为1或0，分别表示词汇表中的单词在输入文档中是否出现\n",
    "    \"\"\"\n",
    "    # 创建一个其中所含元素都为0的向量\n",
    "    return_vector = [0]*len(vocabulary_list)\n",
    "    for word in input_set:\n",
    "        if word in vocabulary_list:\n",
    "            return_vector[vocabulary_list.index(word)] = 1\n",
    "        else:\n",
    "            print('单词：%s 不在词汇表中！' %word)\n",
    "    return return_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_parse(big_string):\n",
    "    list_of_tokens = re.split(r'\\w', big_string)\n",
    "    return [tok.lower() for tok in list_of_tokens if len(tok)>2]\n",
    "\n",
    "\n",
    "def spam_test():\n",
    "    doc_list = []\n",
    "    class_list = []\n",
    "    full_text = []\n",
    "    for i in range(1,26):\n",
    "        # 导入并解析文本文件\n",
    "        file_name = '%d.txt' %i\n",
    "        file_full_path = \\\n",
    "            os.path.join(os.getcwd(),'probability_practice','email','spam',file_name)\n",
    "        with open(file_full_path) as fr:\n",
    "            word_list = text_parse(fr.read())\n",
    "            doc_list.append(word_list)\n",
    "            full_text.append(word_list)\n",
    "            class_list.append(1)\n",
    "        \n",
    "        file_full_path = \\\n",
    "            os.path.join(os.getcwd(),'probability_practice','email','ham',file_name)\n",
    "        with open(file_full_path) as fr:\n",
    "            word_list = text_parse(fr.read())\n",
    "            doc_list.append(word_list)\n",
    "            full_text.append(word_list)\n",
    "            class_list.append(0)\n",
    "    vocabulary_list = create_vocabulary_list(doc_list)\n",
    "    training_set = list(range(50))\n",
    "    test_set= []\n",
    "    \n",
    "    # 随机构建训练集\n",
    "    for i in range(10):\n",
    "        random_index = int(np.random.uniform(0, len(training_set)))\n",
    "        test_set.append(training_set[random_index])\n",
    "        del(training_set[random_index])\n",
    "        \n",
    "    train_mat = []\n",
    "    train_classes = []\n",
    "    for doc_index in training_set:\n",
    "        train_mat.append(set_of_words_2_vector(vocabulary_list, doc_list[doc_index]))\n",
    "        train_classes.append(class_list[doc_index])\n",
    "    \n",
    "    prob_0_vector, prob_1_vector, prob_spam = train_NB1(\n",
    "        np.array(train_mat),\n",
    "        np.array(train_classes)\n",
    "    )    \n",
    "    \n",
    "    error_count = 0\n",
    "    \n",
    "    # 对测试集分类\n",
    "    for doc_index in test_set:\n",
    "        word_vector = set_of_words_2_vector(vocabulary_list, doc_list[doc_index])\n",
    "        if classify_NB(np.array(word_vector), prob_0_vector, prob_1_vector, prob_spam) \\\n",
    "            != class_list[doc_index]:\n",
    "            error_count +=1\n",
    "    print('the error rate is:', float(error_count)/len(test_set))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1:\n-3.14233574775\np0:\n-4.02874727974\np1:\n-16.7737732159\np0:\n-14.0641350341\np1:\n-12.0732928501\np0:\n-9.29345040965\np1:\n-3.14233574775\np0:\n-4.02874727974\np1:\n-8.75910684541\np0:\n-6.85352175515\np1:\n-4.75177366018\np0:\n-7.70081961553\np1:\n-4.75177366018\np0:\n-2.77598431124\np1:\n-6.45652175242\np0:\n-8.7994319042\np1:\n-11.3801456695\np0:\n-12.1838221675\np1:\n-3.14233574775\np0:\n-4.02874727974\nthe error rate is: 0.0\n"
     ]
    }
   ],
   "source": [
    "spam_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
