{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posting_list = [[' my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                    ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                    ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                    ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                    ['mr', 'licks', 'ate', 'my', 'steak', 'how', ' to', 'stop', 'him'],\n",
    "                    ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "\n",
    "# 1代表侮辱性位子，0代表正常言论\n",
    "class_vector = [0, 1, 0, 1, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_list = posting_list\n",
    "return_vector = [0]*len(vocabulary_list)\n",
    "return_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(return_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.zeros((1,6))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_set():\n",
    "    \"\"\"\n",
    "    创建实验样本\n",
    "    :return:    第一个变量是进行词条切分后的文档集合，第二个变量是类别标签的集合\n",
    "    \"\"\"\n",
    "    posting_list = [[' my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                    ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                    ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                    ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                    ['mr', 'licks', 'ate', 'my', 'steak', 'how', ' to', 'stop', 'him'],\n",
    "                    ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "\n",
    "    # 1代表侮辱性位子，0代表正常言论\n",
    "    class_vector = [0, 1, 0, 1, 0, 1]\n",
    "\n",
    "    return posting_list, class_vector\n",
    "\n",
    "\n",
    "def create_vocabulary_list(data_set):\n",
    "    \"\"\"\n",
    "    创建一个包含在所有文档中出现的不重复词的列表\n",
    "    :param data_set:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 创建一个空集\n",
    "    vocabulary_set = set([])\n",
    "    for document in data_set:\n",
    "        # 创建两个集合的并集\n",
    "        vocabulary_set = vocabulary_set | set(document)\n",
    "    return list(vocabulary_set)\n",
    "\n",
    "\n",
    "def set_of_words_2_vector(vocabulary_list, input_set):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param vocabulary_list:     词汇的列表\n",
    "    :param input_set:           某个文档\n",
    "    :return:                    文档向量，向量的每一个元素为1或0，分别表示词汇表中的单词在输入文档中是否出现\n",
    "    \"\"\"\n",
    "    # 创建一个其中所含元素都为0的向量\n",
    "    return_vector = [0]*len(vocabulary_list)\n",
    "    for word in input_set:\n",
    "        if word in vocabulary_list:\n",
    "            return_vector[vocabulary_list.index(word)] = 1\n",
    "        else:\n",
    "            print('单词：%s 不在词汇表中！' %word)\n",
    "    return return_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['garbage',\n 'take',\n 'not',\n 'park',\n 'cute',\n 'I',\n 'how',\n 'please',\n 'food',\n 'help',\n 'him',\n 'quit',\n 'flea',\n 'is',\n 'so',\n 'maybe',\n 'stop',\n 'buying',\n 'worthless',\n ' to',\n 'mr',\n 'problems',\n 'love',\n 'my',\n 'steak',\n 'has',\n 'licks',\n 'to',\n 'stupid',\n 'dalmation',\n 'ate',\n ' my',\n 'dog',\n 'posting']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_o_posts,list_classes = load_data_set()\n",
    "my_vocab_list = create_vocabulary_list(list_o_posts)\n",
    "my_vocab_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 伪代码\n",
    "计算每个类别中的文档数目  \n",
    "（例如上边的posting_list和class_vector，每个类别中的文档数目都应该是3？）  \n",
    "\n",
    "1. 对每篇训练文档：  P(W0|类别1),P(W1|类别1),P(W2|类别1)...P(W|类别1)  \n",
    "1.1 对每个类别：  \n",
    "1.1.1 如果词条出现在文档中 -> 增加该词条的计数值  \n",
    "1.1.2 增加所有词条的计数值  \n",
    "1.2 对每个类别:  \n",
    "1.2.1 对每个词条:  \n",
    "1.2.1.1 将该词条的数目除以总词条数目得到条件概率  \n",
    "1.3 返回每个类别的条件概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_NB0(train_matrix:list, train_category):\n",
    "    \"\"\"\n",
    "\n",
    "    :param train_matrix:\n",
    "    :param train_category:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 训练文档的数目\n",
    "    num_train_docs = len(train_matrix)\n",
    "    num_words = len(train_matrix[0])\n",
    "\n",
    "    # 分类为\"侮辱性文档\"的概率: 分类为\"侮辱性文档\"的文档数目，除以训练文档的总数目\n",
    "    # train_category 为一维向量，只有0和1两种值，其中1代表侮辱性\n",
    "    p_abusive = sum(train_category)/float(num_train_docs)\n",
    "\n",
    "    # 初始化概率：\n",
    "    p0_num = np.zeros(num_words)\n",
    "    p1_num = np.zeros(num_words)\n",
    "\n",
    "    # 这两个是充当分母的？\n",
    "    p0_denom = 0.0\n",
    "    p1_denom = 0.0\n",
    "\n",
    "    for i in range(num_train_docs):\n",
    "        if train_category[i] == 1:  # 类别为\"侮辱\"\n",
    "            p1_num += train_matrix[i]\n",
    "            p1_denom += sum(train_matrix[i])    # 训练文档中在词汇表中存在的单词数目\n",
    "        else:\n",
    "            p0_num += train_matrix[i]\n",
    "            p0_denom += sum(train_matrix[i])    # 训练文档中在词汇表中存在的单词数目\n",
    "\n",
    "    p1_vector = p1_num/p1_denom # 需要转为对数\n",
    "\n",
    "    # 对每个元素做除法\n",
    "    p0_vector = p0_num/p0_denom # 需要转为对数\n",
    "\n",
    "    return p0_vector, p1_vector, p_abusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# my_vocab_list\n",
    "train_mat = []\n",
    "for post_in_doc in list_o_posts:\n",
    "    train_mat.append(set_of_words_2_vector(my_vocab_list, post_in_doc))\n",
    "type(train_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 1, 0, 1]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0_v, p1_v, p_ab = train_NB0(train_mat, list_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "任意文档属于侮辱性文档的概率："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_ab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在非侮辱性文档类别下词汇表中单词的出现概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.04166667,\n        0.04166667,  0.04166667,  0.04166667,  0.        ,  0.04166667,\n        0.08333333,  0.        ,  0.04166667,  0.04166667,  0.04166667,\n        0.        ,  0.04166667,  0.        ,  0.        ,  0.04166667,\n        0.04166667,  0.04166667,  0.04166667,  0.08333333,  0.04166667,\n        0.04166667,  0.04166667,  0.        ,  0.        ,  0.04166667,\n        0.04166667,  0.04166667,  0.04166667,  0.        ])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在侮辱性文档类别下词汇表中单词的出现概率向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05263158,  0.05263158,  0.05263158,  0.05263158,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.05263158,  0.        ,\n        0.05263158,  0.05263158,  0.        ,  0.        ,  0.        ,\n        0.05263158,  0.05263158,  0.05263158,  0.10526316,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.05263158,  0.15789474,  0.        ,\n        0.        ,  0.        ,  0.10526316,  0.05263158])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试算法：根据现实情况修改分类器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 计算p(w0|1)p(w1|1)p(w2|1)...如果其中一个概率值为0，那么所有乘积也为0，需降低这种影响\n",
    "2. 计算p(w0|1)p(w1|1)p(w2|1)...,由于大部分银子都非常小，所以这个乘积将会下溢出或者得不到正确的答案。一种解决办法是对乘积取自然数对数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_NB1(train_matrix, train_category):\n",
    "    \"\"\"\n",
    "    train_NB0函数的改进\n",
    "    :param train_matrix:    训练文档矩阵（准确来说只是python原生二维数组）\n",
    "    :param train_category:  训练文档矩阵对应的分类（一维向量）\n",
    "    :return: (在非侮辱性文档类别下词汇表中单词的出现概率向量, 在侮辱性文档类别下词汇表中单词的出现概率向量, 任意文档属于侮辱性文档的概率)\n",
    "    \"\"\"\n",
    "    # 训练文档的数目\n",
    "    num_train_docs = len(train_matrix)\n",
    "    num_words = len(train_matrix[0])\n",
    "\n",
    "    # 分类为\"侮辱性文档\"的概率: 分类为\"侮辱性文档\"的文档数目，除以训练文档的总数目\n",
    "    # train_category 为一维向量，只有0和1两种值，其中1代表侮辱性\n",
    "    p_abusive = sum(train_category) / float(num_train_docs)\n",
    "\n",
    "    # 初始化概率：初始化为1而不是0\n",
    "    # p0_num: 对于分类0，词汇表中单词的出现次数\n",
    "    # p1_num: 对于分类1，词汇表中单词的出现次数\n",
    "    p0_num = np.ones(num_words)\n",
    "    p1_num = np.ones(num_words)\n",
    "\n",
    "    # 这两个是充当分母，\n",
    "    # TODO 为什么是2？为什么不是1或者其他数？\n",
    "    p0_denom = 2.0\n",
    "    p1_denom = 2.0\n",
    "\n",
    "    for i in range(num_train_docs):\n",
    "        if train_category[i] == 1:  # 类别为\"侮辱\"\n",
    "            p1_num += train_matrix[i]\n",
    "            p1_denom += sum(train_matrix[i])  # 训练文档中的单词同时也在词汇表中存在的数目\n",
    "        else:\n",
    "            p0_num += train_matrix[i]\n",
    "            p0_denom += sum(train_matrix[i])  # 训练文档中的单词同时也在词汇表中存在的数目\n",
    "\n",
    "    # 概率向量(转为对数)\n",
    "    # 分母是，对于分类1（侮辱性文档），一共出现过多少个单词（所有文档中）\n",
    "    # 分子是，对于分类1（侮辱性文档），词汇表中每个单词各自出现的次数（所有文档中）\n",
    "    # 相除后，得到一个对于分类1（侮辱性文档），词汇表中每个单词出现的概率\n",
    "    p1_vector = np.log(p1_num / p1_denom)\n",
    "\n",
    "    # 对每个元素做除法\n",
    "    p0_vector = np.log(p0_num / p0_denom)\n",
    "\n",
    "    return p0_vector, p1_vector, p_abusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_NB(vector_2_classify, p0_vector, p1_vector,p_class_1):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param vector_2_classify:   要分类的向量 \n",
    "    :param p0_vector:           在非侮辱性文档类别下词汇表中单词的出现概率向量\n",
    "    :param p1_vector:           在侮辱性文档类别下词汇表中单词的出现概率向量\n",
    "    :param p_class_1:           任意文档属于侮辱性文档的概率\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    p1 = sum(vector_2_classify * p1_vector) + np.log(p_class_1)\n",
    "    p0 = sum(vector_2_classify * p0_vector) + np.log(1.0 - p_class_1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯分类函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list_o_posts 训练集：文档列表  \n",
    "list_classes 训练集对应的分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[' my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n  ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n  ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n  ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n  ['mr', 'licks', 'ate', 'my', 'steak', 'how', ' to', 'stop', 'him'],\n  ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']],\n [0, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_o_posts, list_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**my_vocab_list**：根据list_o_posts中训练集所有文档，抽取所有曾出现的单词，得出的词汇列表  \n",
    "\n",
    "**train_mat**：list_o_posts中训练集文档，原来是多组组由若干单词组成的向量，现在把它转换为若干组由(1,0)组成的向量，遍历my_vocab_list中的单词，如果该单词在文档中出现，则标记为1，没出现则标记为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['garbage',\n  'take',\n  'not',\n  'park',\n  'cute',\n  'I',\n  'how',\n  'please',\n  'food',\n  'help',\n  'him',\n  'quit',\n  'flea',\n  'is',\n  'so',\n  'maybe',\n  'stop',\n  'buying',\n  'worthless',\n  ' to',\n  'mr',\n  'problems',\n  'love',\n  'my',\n  'steak',\n  'has',\n  'licks',\n  'to',\n  'stupid',\n  'dalmation',\n  'ate',\n  ' my',\n  'dog',\n  'posting'],\n [[0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   1,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   0],\n  [0,\n   1,\n   1,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   0,\n   0,\n   0,\n   1,\n   0],\n  [0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   1,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0],\n  [1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   1],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   1,\n   1,\n   0,\n   0,\n   1,\n   1,\n   0,\n   1,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0],\n  [0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   1,\n   0]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_vocab_list, train_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_mat，list_classes的类型为python原生的多维数组，现在把它们转换为numpy中array类型的变量：train_array，list_classes_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_array = np.array(train_mat)\n",
    "list_classes_array = np.array(list_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**prob_0_vector**：训练集中类别为\"非侮辱性\"的文档中，词汇表中每个单词的出现概率（组成一个一维向量）  \n",
    "暂且理解为公式中的p(w|分类0)\n",
    "\n",
    "**prob_1_vector**：训练集中类别为\"侮辱性\"的文档中，词汇表中每个单词的出现概率（组成一个一维向量）  \n",
    "暂且理解为公式中的p(w|分类1)\n",
    "\n",
    "**prob_abusive**：训练集中类别为\"侮辱性\"文档的出现概率 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_0_vector, prob_1_vector, prob_abusive = train_NB1(train_array, list_classes_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试集 文档列表\n",
    "test_entry = ['love', 'my', 'dalmation']\n",
    "# test_entry = ['stupid','garbage']\n",
    "# 转换测试集的格式，转化为词汇表中单词是否出现组成的向量\n",
    "test_mat = set_of_words_2_vector(my_vocab_list, test_entry)\n",
    "# 把转换后的测试集，再转为numpy中的array\n",
    "test_doc_array = np.array(test_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算测试集（只有1条数据）的概率\n",
    "\n",
    "**这公式tmd怎么来的？！**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试集的那条数据，类别可能为\"侮辱性\"的概率\n",
    "test_prob_1 = sum(test_doc_array * prob_1_vector) + np.log(prob_abusive)\n",
    "\n",
    "# 测试集的那条数据，类别可能为\"非侮辱性\"的概率\n",
    "test_prob_0 = sum(test_doc_array * prob_0_vector) + np.log(1.0 - prob_abusive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-9.8267144937302149, -7.9825301448363906)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prob_1, test_prob_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 上边的都是词集模型，现在进化一下，改为词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_2_vector(vocabulary_list, input_set):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param vocabulary_list:     词汇的列表\n",
    "    :param input_set:           某个文档\n",
    "    :return:                    文档向量，向量的每一个元素整数，表示单词出现次数\n",
    "    \"\"\"\n",
    "    # 创建一个其中所含元素都为0的向量\n",
    "    return_vector = [0]*len(vocabulary_list)\n",
    "    for word in input_set:\n",
    "        if word in vocabulary_list:\n",
    "            return_vector[vocabulary_list.index(word)] += 1\n",
    "        else:\n",
    "            print('单词：%s 不在词汇表中！' %word)\n",
    "    return return_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 乱写乱画  \n",
    "\n",
    "最终目的：求p(分类1|w),p(分类2|w)  \n",
    "\n",
    "由贝叶斯公式可知：  \n",
    "【向量为w 与 分类为1 同时发生的概率】 = p(w)\\*p(分类1) = p(w)\\*p(分类1|w) = p(w|分类1) \\* p(分类1)  \n",
    "\n",
    "因此p(分类1|w) = p(w|分类1) \\* p(分类1) / p(w)  \n",
    "\n",
    "而因为w是由w0,w1,w2,w3....组成的一组向量，由此可得  \n",
    "p(w) = p(w0) \\* p(w1) \\* p(w2) \\* p(w3) ...  \n",
    "p(w|分类1) = p(w0|分类1) \\* p(w1|分类1) \\* p(w2|分类1) \\* p(w3|分类1) ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAHMFJREFUeJzt3Xl0XFeB5/HvlWRZi7Xv1u41El5iW7GdjWxmcUIwCUkakrAGDIEMDPTQDSRAutM908CEOWdCpmnTYSAhEMISE4gZEmchm7PIlh3b8ibL1mZZ+76r6s4fVVZMkC3ZKtWrV/X7nKNjlfRc9buW9PPVfa9uGWstIiISPqKcDiAiIoGlYhcRCTMqdhGRMKNiFxEJMyp2EZEwo2IXEQkzKnYRkTCjYhcRCTMqdhGRMBPjxINmZmbakpISJx5aRMS1du7c2W6tzZrqOEeKvaSkhMrKSiceWkTEtYwxddM5TksxIiJhRsUuIhJmVOwiImFGxS4iEmZU7CIiYSYgxW6Meb8x5pAxpsYY8/VA3KeIiJyfGRe7MSYaeBDYCJQDHzXGlM/0fkVE5PwE4jr2tUCNtbYWwBjzGLAJqA7AfYuIuJa1lo6BUY63D3C8Y5C6jgFuqSikMD1hVh83EMWeDzScdrsRWPfOg4wxm4HNAEVFRQF4WBER550q72PtAxxrH6CuY4Dj7YMc7xigrmOQ/pHxiWOjDKwuSnNFsU+LtXYLsAWgoqJCr6AtIq4yMDI+Ud61bQMca+/3vd8+QN/w2+UdHWUoTIunJDORi0rSKc5IoCQjkeKMBArSEoiNmf1rVgJR7E1A4Wm3C/wfExFxlTGPl4bOQX9x+0r7VIG39I781bH5qfGUZiZyw6p8SjMTKclMZEFmIvNT45kT7ewFh4Eo9jeBxcaYUnyF/hHg1gDcr4hIwFlrae0b4Whrv7+4336r7xzE4317QSE9MZbSzEQuX5xFqb+4S7MSKclIJG5OtIOjOLsZF7u1dtwYcxfwZyAa+Im1dv+Mk4mIzIDHa6nvHKSmtZ+a1n6Otvn/bO2n77R177g5UZRmzqM8L5nrludR6i/vBZmJpCbEOjiC8xeQNXZr7TZgWyDuS0TkXAyPeahtG6DmtOKuafUtn4x6vBPHZSfNZVH2PG5Ync+i7HkszJrHgqxEcpLiiIoyDo4g8BzZtldE5Fz1DI1R09o3MQP3zcIHaOgaxPpXT6IMFKYnsChrHlcuzWJh9ryJEk+Jn+PsAIJIxS4iIWVo1ENNaz+HWvo43NLHoZO+P5t7hieOiY2JYkFmIisKUrjxtBl4aWZor30Hi4pdRBwx5vFyrH1gorhP/VnX+fYMPDYmikVZ81i/IIMlOUksyfHNwAvSEogOs+WTQFKxi8is8notDV2Dbxd4Sz+HT/ZR297PmMfX4NFRhpKMBMrnJ/OhVfkszUliSW4SxekJxDh86aAbqdhFJGAGRsY5eLKPA829HGjupbq5l0Mn+xgc9UwcU5AWz9KcJK4uy/YVeE4SC7K0hBJIKnYROWfWWk70DHPghK/AD5zspfpE718toyTFxVCWl8wtFYVckJvE0twkFuckMW+uame26V9YRM5qZNzDkZZ+qv2zcN9bHz1DYxPHFGckUJabzI2rCyjLS6YsL4n81HiM0Tq4E1TsIjJheMxDdXMv+5p62NfUw96mXo609DHufzZm/JxoluYmce3yPMrnJ1Oel8TS3GTNwkOMvhoiEWpwdJwDzb3sbfQV+P4TPRxp7Z94Sn16YizL8lO4ammWv8STKc5I1NUoLqBiF4kAAyPjVPtL3DcT7+FoWz+ntkXJnOcr8feU57AsP4Xl+SnkpcRpKcWlVOwiYWbM4+XQyT6qGrrZ09DN7oZujrb1T5zUzE6ay/L8FDYuz2O5v8RzkueqxMOIil3Exay1NHYNUdXQze76bvY0drOvqYeRcd8eKRmJsVxYmMoHVrxd4tnJcQ6nltmmYhdxke7BUfY09kyU+J6GbjoGRgGYGxPF8vwUbl9fzIWFqVxYmEpBmq5MiUQqdpEQ5fVaatr6qTzexc66LnbVd3GsfQAAY2BR1jyuviCblf4SX5qb5PgLPEhoULGLhIjB0XF2N3Szq66LyroudtV10et/ybWMxFhWFaVx05oCVhWmsrwghaS4yNmtUM6Nil3EIc09Q+ys66LyuG82vv9E78Slhkty5nHdijzWFKezpjiNkowELanItKnYRYLAWsvRtgFeP9bBG8c6qTzeRVP3EOB70s/KwhTuvGIha0rSWF2YRkqCZuNy/lTsIrPA67Ucae3n9WMdvF7byevHOmnv970YcnbSXC4qTeczl5eypjiNsrxkrY1LQKnYRQLA67UcONnrL3HfrLxr0LeXyvyUOC5fnMm60nTWLcjQsorMOhW7yHmw1nK4pZ+Xa9rZcbSdN451TpzoLEyP55qyHNaVprN+QYYuOZSgU7GLTFNT9xCv1LT73zomllZKMhK4dnke6xaks640g/mp8Q4nlUinYhc5g57BMXbUtvNyTTuv1nRQ67+GPHNeLJcuypx4y1eRS4hRsYv4jXm87Kzr4sXDbbxS087eph68FhJjo1m3IIPb1hdz2aJMluTM09KKhDQVu0S05p4h/nKojRcO+cq8b2ScmCjDqqJUvnTNYi5blMnKwlRdtSKuomKXiDI67qWyrnOizA+19AGQlxLHB1bmccWSbC5dlKFndYqrqdgl7LX0DvPsgVZeONTKKzXtDIx6mBNtuKgknW+uuYArl2azOFvLKxI+VOwSdqy1HDzZx/bqFrYfaGFPYw8A+anxfGhVPlcuzebihRl6OTcJW/rOlrAw5vHyxrFOnvGXeWPXEMbAhYWpfO19S3lPeY5m5RIxVOziWn3DYzx/qI3t1S08f6iVvuFx5sZEcfniTO66ahFXl2WTnaQXlZDIo2IXV+kZGuPZAy1s29vMi4fbGfV4yUiMZeOyXDaU5XDZ4kwSYvVtLZFNPwES8roHR3m6uoU/7W3m5Zp2xjyWvJQ4bl9fzMbluawuSiM6SkssIqeo2CUkdQ6M8vT+k2zbd5JXa9oZ91oK0uL51KWlbFyWy8qCVKJU5iKTUrFLyBgcHeeZ6hZ+v/sELx5uY9xrKUpP4DOXL+Da5bksz0/RyU+RaVCxi6PGPF5ePtLO1t1NPL2/haExD/NT4rjj8lKuXzGfd81PVpmLnKMZFbsx5mbgXqAMWGutrQxEKAlv1lp21XexteoET+1tpnNglJT4OdywOp9NK+dzUUm6lllEZmCmM/Z9wI3AfwQgi4S5pu4hfruzkd/sbKS+c5C4OVFsKMth04X5XLEki9gY7cciEggzKnZr7QFAvyrLGQ2PeXi6uoVfVzbwck071sIlCzP48jWLed+yXD37U2QWBO2nyhizGdgMUFRUFKyHFQdYa9nb1MOvKxv5/e4meofHyU+N50tXL+amNQUUpic4HVEkrE1Z7MaY7UDuJJ+621r7++k+kLV2C7AFoKKiwk47obhG3/AYW6uaePT1eg6e7GNuTBTvX5bLLRWFXLwgQ+vmIkEyZbFbazcEI4i4V/WJXn7+eh1bq5oYHPWwLD+Zf/nQMq5fOZ+UeG1/KxJsWuCU8zI85mHb3mZ+/lodu+q7mRsTxQdXzue29cWsLND15iJOmunljjcADwBZwFPGmN3W2vcFJJmEpBPdQ/xsx3Eef7OBrsExFmQm8q0PlHPT6gJSEjQ7FwkFM70q5gngiQBlkRBWVd/FQy8f40/7TgLw3vIcPra+mIsXZmh2LhJitBQjZzTu8fLn/S089HItu+q7SYqL4Y7LSvnEJSXkp8Y7HU9EzkDFLn+jf2Scx96o5/++cpym7iGKMxK49/pybqoo1HXnIi6gn1KZ0DUwyk9fPc5PXz1Oz9AYa0vT+fb15Wwoy9G2uCIuomIXWnqH+c+Xann09XoGRz28pzyHL1y5kFVFaU5HE5HzoGKPYPUdg/zoxaP8prKRca+XD66cz51XLmJpbpLT0URkBlTsEaipe4gHnj3Cr3c2Em0MN1UU8Ll3L6A4I9HpaCISACr2CNLSO8yDz9fw2BsNANy+rog7r1xEbope8FkknKjYI0B7/wg/euEoj7xWh8drubmigLuuXqxLFkXClIo9jA2OjrPlxVq2vFjL8JiHG1YV8OVrFlOUod0VRcKZij0MebyW3+xs4P6nD9PaN8LGZbn8/XuXsih7ntPRRCQIVOxh5i+H2/gf2w5w8GQfq4pS+ffbV7OmON3pWCISRCr2MFHT2sc//aGal460U5SewIO3ruba5bnax0UkAqnYXa5/ZJwHnj3CQy8fIz42mnuuK+NjFxczNyba6Wgi4hAVu0tZa/nDW83861PVtPSOcPOaAv5x4wVkzpvrdDQRcZiK3YVqWvv41tb97Kjt4F3zk/k/t61hTbGe/i8iPip2Fxkd9/Kjvxzlh8/VEB8bzX0fWsata4u0QZeI/BUVu0vsaejmH3/7FgdP9nH9yvl85/pyLbuIyKRU7CFuaNTDD545xEMvHyM7KY7//HgFG8pznI4lIiFMxR7CdtV38dVf7eZ4xyC3rivi6xsvIDlOrysqImenYg9BYx4vDzxXw4PP15CbHMcvPruOSxZmOh1LRFxCxR5iatv6+cqvdrOnsYcbV+Vz76Z3aZYuIudExR4irLX86s0G/ukP1cTGRPHgrau5bkWe07FExIVU7CFgYGSce7bu44mqJi5blMn/vHml9kgXkfOmYnfY4ZY+vvDoLmrb+vnqe5bwxasW6bp0EZkRFbuDfruzkXu27iNxbgw/v2MdlyzSCVIRmTkVuwPGPF7+5Y/V/GxHHetK03ngo6vITtbSi4gEhoo9yLoGRvniL3bx6tEOPnNZKV/feAEx0VFOxxKRMKJiD6IjLX185uFKmruH+f5NK7i5otDpSCIShlTsQfLCoVbu+kUVcXOi+eXm9dqNUURmjYo9CB6vbOAbv9vL0pwkHvpkBXkp8U5HEpEwpmKfRdZafvhcDfc/c5jLF2fy77evYd5c/ZOLyOxSy8yScY+Xbz+5n1+8Xs+Nq/L5tw+vIDZGJ0lFZPap2GfBmMfLlx+rYtvek3zhyoV87X1L9aLSIhI0Myp2Y8z3geuBUeAo8ClrbXcggrnVyLiHLz5axfYDLdxzXRmfuXyB05FEJMLMdG3gGWCZtXYFcBj4xswjudfwmIfPPryT7QdauG/Tu1TqIuKIGRW7tfZpa+24/+ZrQMHMI7nT0KiHT//0TV460sZ3P7ycj11c4nQkEYlQgTyb92ngTwG8P9cYGfew+ZFKXqvt4Ae3rOTvLipyOpKIRLAp19iNMduB3Ek+dbe19vf+Y+4GxoFHz3I/m4HNAEVF4VN84x4vX/plFS8daed7N63ghlUR+0uLiISIKYvdWrvhbJ83xnwS+ABwjbXWnuV+tgBbACoqKs54nJt4vZav/eYt/ry/he9cX84t2iJARELATK+KeT/wD8AV1trBwERyB2st9/5hP09UNfHf3ruET11a6nQkERFg5mvsPwSSgGeMMbuNMT8KQCZX+PFLtTy8o47PXl7KF69a5HQcEZEJM5qxW2sjstGeequZ/77tINctz+MbG8v05CMRCSl6jvs52lnXyVce382a4jTuv2UlUXoZOxEJMSr2c9DYNchnH97J/JQ4fvzxCuLmRDsdSUTkb6jYp2l4zMPnHtnJmMfLTz55EemJsU5HEhGZlDYBmwZrLd98Yi/7T/Ty0CcqWJA1z+lIIiJnpBn7NDy8o47f7Wriv25YzDVlOU7HERE5KxX7FKrqu7jvj9VsKMvmS1cvdjqOiMiUVOxn0Tc8xpceqyInOY77b7lQV8CIiCtojf0svrV1H01dQzz+uYtJiZ/jdBwRkWnRjP0Mfrerka27T/Dla5ZQUZLudBwRkWlTsU+isWuQb23dx9qSdO66OiKfXCsiLqZifwdrLd/43V4AfvB3K4nWurqIuIyK/R1+XdnIS0fa+fq1ZRSkJTgdR0TknKnYT3OyZ5j7nqpmXWk6t60NnxcDEZHIomI/zT1b9zHm8fLdD6/QpY0i4loqdr9nD7Sw/UALX9mwhJLMRKfjiIicNxU7vg2+/vmP1SzMStQrIYmI6+kJSsCPX6ylrmOQR+5YS2yM/q8TEXeL+BZr7BrkwRdq2Lgsl8sXZzkdR0RkxiK+2O9/+jDWwj0fKHc6iohIQER0sVef6GXr7iY+dWkp+anxTscREQmIiC727/35IElzY7jzioVORxERCZiILfbXajt44VAbX7xqESkJ2rlRRMJHRBa7tZbv/b+D5KXE8YlLSpyOIyISUBFZ7DuOdrCrvpsvXLWIuDnRTscREQmoiCz2B56rITtpLjevKXA6iohIwEVcse+s62JHbQeb371As3URCUsRV+wPPl9DemIst67T7o0iEp4iqtgPnezjuYOtfPrSEhJitZuCiISniCr2n756nLg5Udy2rtjpKCIisyZiir17cJQnqhq5YVU+aYmxTscREZk1EVPsj73ZwPCYV9eti0jYi4hiH/d4eWRHHRcvyOCC3GSn44iIzKqIKPbnD7XR1D2k2bqIRISIKPbHKxvISprLhrJsp6OIiMy6GRW7MeY+Y8xbxpjdxpinjTHzAxUsUFr7hnnuYCs3rs4nJjoi/h8TkQg306b7vrV2hbX2QuCPwLcDkCmgtlY14fFabl5T6HQUEZGgmFGxW2t7T7uZCNiZxQksay2PVzaypjiNRdnznI4jIhIUM16bMMb8qzGmAbiNEJuxv9XYQ01rvzb7EpGIMmWxG2O2G2P2TfK2CcBae7e1thB4FLjrLPez2RhTaYypbGtrC9wIzuIPe04QGx3FxuV5QXk8EZFQMOWGKdbaDdO8r0eBbcB3znA/W4AtABUVFbO+ZOP1WrbtbebdSzJJidcrJIlI5JjpVTGLT7u5CTg4sziBU9XQxYmeYa5bodm6iESWmW5x+G/GmKWAF6gDPj/zSIHxx7eaiY2JYkNZjtNRRESCakbFbq39cKCCBNKpZZgrl2SRFKdlGBGJLGH5jJ23mnpo6R3hWp00FZEIFJbF/tyBFqIMXLk0y+koIiJBF5bF/uzBViqK00lN0L7rIhJ5wq7YT/YMs/9EL1ddoA2/RCQyhV2xP3ewFYBrtJOjiESosCz2grR4FmtvGBGJUGFV7OMeL6/VdvDuJVkYY5yOIyLiiLAq9r1NPfSPjHPpwkyno4iIOCasiv3Vox0ArF+Q7nASERHnhFWx7zjawQW5SWTMm+t0FBERx4RNsY+Me3jzeCcXL8xwOoqIiKPCptir6rsZGfdyidbXRSTChU2xv3GsE4C1pVpfF5HIFjbFvqu+i8XZ8/SiGiIS8cKi2K21VNV3s7oozekoIiKOC4tir20foGdojNXFqU5HERFxXFgUe1V9NwCrNGMXEQmPYt9V30VSXAyLsrQ/jIhIWBR7VX03FxamEhWl/WFERFxf7MNjHg639LGiIMXpKCIiIcH1xX6kpR+P1/Ku+Sp2EREIg2Kvbu4BoDwv2eEkIiKhwf3FfqKXxNhoitITnI4iIhIS3F/szb2U5SXrxKmIiJ+ri93rtRxo7qN8vpZhREROcXWxN3QN0j8yrvV1EZHTuLrYD57sA+ACFbuIyARXF3tNaz8AC7MSHU4iIhI6XF3sR9v6yU2OIylOW/WKiJzi8mIfYGG2ZusiIqdzbbFba6lt7WehNv4SEfkrri321r4R+kbGWZStYhcROZ1ri/3oxIlTFbuIyOncW+xtKnYRkckEpNiNMX9vjLHGmMxA3N901LYPkBAbTU7y3GA9pIiIK8y42I0xhcB7gfqZx5m+hs4hCtMSMEZ7xIiInC4QM/b/BfwDYANwX9PW2DVIYXp8MB9SRMQVZlTsxphNQJO1dk+A8kyLtZaGzkEKtVWviMjfiJnqAGPMdiB3kk/dDXwT3zLMlIwxm4HNAEVFRecQ8W91DY4xMOqhME3FLiLyTlMWu7V2w2QfN8YsB0qBPf517gJglzFmrbX25CT3swXYAlBRUTGjZZv6zkEAzdhFRCYxZbGfibV2L5B96rYx5jhQYa1tD0Cus2qYKHatsYuIvJMrr2Nv6PIXu5ZiRET+xnnP2N/JWlsSqPuaSkPnEOmJsSTODVh8EZGw4coZe3PPEPNT45yOISISklxZ7C29I+Qmq9hFRCbj0mIfJkfFLiIyKdcV+8i4h86BURW7iMgZuK7YW3tHALQUIyJyBq4r9pbeYQByUlTsIiKTcV2xnzxV7NquV0RkUu4r9h5fsWspRkRkcq4r9ta+EebGRJESP8fpKCIiIcl1xX7qUke9wIaIyORcV+zt/SNkzot1OoaISMhyXbF39I+SnqgTpyIiZ+K6Yu8cGCUjUTN2EZEzcVWxW2vpGhwlXUsxIiJn5Kpi7x0eZ8xjNWMXETkLVxV758AoAOkqdhGRM3JZsfv2iVGxi4icmauKvaPfN2PP0FUxIiJn5Kpin1iK0clTEZEzclWxd5wq9gQVu4jImbiq2DsHRomfE018bLTTUUREQparin1x9jyuX5nndAwRkZAW43SAc/GRtUV8ZG2R0zFEREKaq2bsIiIyNRW7iEiYUbGLiIQZFbuISJhRsYuIhBkVu4hImFGxi4iEGRW7iEiYMdba4D+oMW1A3Tn8lUygfZbihLJIHHckjhkic9yROGaY2biLrbVZUx3kSLGfK2NMpbW2wukcwRaJ447EMUNkjjsSxwzBGbeWYkREwoyKXUQkzLil2Lc4HcAhkTjuSBwzROa4I3HMEIRxu2KNXUREps8tM3YREZmmkCp2Y8z7jTGHjDE1xpivT/L5ucaYX/k//7oxpiT4KQNrGmP+qjGm2hjzljHmWWNMsRM5A22qcZ923IeNMdYY4/qrJ6YzZmPMLf6v935jzC+CnXE2TON7vMgY87wxpsr/fX6tEzkDyRjzE2NMqzFm3xk+b4wx/9v/b/KWMWZ1QANYa0PiDYgGjgILgFhgD1D+jmO+APzI//5HgF85nTsIY74KSPC/f6fbxzzdcfuPSwJeBF4DKpzOHYSv9WKgCkjz3852OneQxr0FuNP/fjlw3OncARj3u4HVwL4zfP5a4E+AAdYDrwfy8UNpxr4WqLHW1lprR4HHgE3vOGYT8DP/+78BrjHGmCBmDLQpx2ytfd5aO+i/+RpQEOSMs2E6X2uA+4DvAsPBDDdLpjPmzwIPWmu7AKy1rUHOOBumM24LJPvfTwFOBDHfrLDWvgh0nuWQTcDD1uc1INUYE7DX/QylYs8HGk673ej/2KTHWGvHgR4gIyjpZsd0xny6O/D9L+92U47b/6tpobX2qWAGm0XT+VovAZYYY14xxrxmjHl/0NLNnumM+17gdmNMI7AN+C/Bieaoc/3ZPyeues3TSGaMuR2oAK5wOstsM8ZEAT8APulwlGCLwbcccyW+38xeNMYst9Z2O5pq9n0U+Km19n5jzMXAI8aYZdZar9PB3CqUZuxNQOFptwv8H5v0GGNMDL5f2zqCkm52TGfMGGM2AHcDH7TWjgQp22yaatxJwDLgBWPMcXxrkE+6/ATqdL7WjcCT1toxa+0x4DC+onez6Yz7DuBxAGvtDiAO334q4WxaP/vnK5SK/U1gsTGm1BgTi+/k6JPvOOZJ4BP+928CnrP+MxEuNeWYjTGrgP/AV+rhsOYKU4zbWttjrc201pZYa0vwnVv4oLW20pm4ATGd7++t+GbrGGMy8S3N1AYz5CyYzrjrgWsAjDFl+Iq9Lagpg+9J4OP+q2PWAz3W2uaA3bvTZ48nOVN8GN9Z9Lv9H/tnfD/U4PuC/xqoAd4AFjidOQhj3g60ALv9b086nTkY437HsS/g8qtipvm1NviWoKqBvcBHnM4cpHGXA6/gu2JmN/BepzMHYMy/BJqBMXy/id0BfB74/Glf6wf9/yZ7A/39rWeeioiEmVBaihERkQBQsYuIhBkVu4hImFGxi4iEGRW7iEiYUbGLiIQZFbuISJhRsYuIhJn/D8H4o/cBhiTvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x106b44780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot\n",
    "\n",
    "x_ = np.linspace(0.01, 1, 10000)\n",
    "y_ = np.log(x_)\n",
    "\n",
    "plt.plot(x_,y_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
